{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgpUTT+g+8P5bi2eT8tJPL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhishek21599/Deep-learning/blob/main/transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YshwYBUqOxDj"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "vgsREpBSPEP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications import ResNet50\n",
        "# load the model\n",
        "model = ResNet50()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "e6NPXMSNPI9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let us load an image to test the pretrained VGG model.\n",
        "#These models are developed on powerful computers so we may as well use them for transfer learning\n",
        "#For VGG16 the images need to be 224x224.\n",
        "from keras.preprocessing.image import load_img\n",
        "image = load_img('/content/drive/MyDrive/FullSizeRender.jpg', target_size=(224, 224))\n",
        "\n",
        "#Convert pixels to Numpy array\n",
        "from keras.preprocessing.image import img_to_array\n",
        "image = img_to_array(image)\n",
        "\n",
        "# Reshape data for the model. VGG expects multiple images of size 224x224x3,\n",
        "#therefore the input shape needs to be (1, 224, 224, 3)\n",
        "#image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "import numpy as np\n",
        "image = np.expand_dims(image, axis=0)\n",
        "\n",
        "#Data needs to be preprocessed same way as the training dataset, to get best results\n",
        "#preprocessing from Keras does this job.\n",
        "#Notice the change in pixel values (Preprocessing subtracts mean RGB value of training set from each pixel)\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "image = preprocess_input(image)\n",
        "# predict the probability across all output categories.\n",
        "#Probability for each of the 1000 classes will be calculated.\n",
        "pred = model.predict(image)\n",
        "\n",
        "#Print the probabilities of the top 5 classes\n",
        "from tensorflow.keras.applications.mobilenet import decode_predictions\n",
        "pred_classes = decode_predictions(pred, top=5)\n",
        "for i in pred_classes[0]:\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "1-hkmMRTPODX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Conv2D, UpSampling2D, Input\n",
        "from keras.models import Sequential, Model\n",
        "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "from skimage.color import rgb2lab, lab2rgb, gray2rgb\n",
        "from skimage.transform import resize\n",
        "from skimage.io import imsave\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import os"
      ],
      "metadata": {
        "id": "YI2OcCWhPVv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "vggmodel = VGG16()"
      ],
      "metadata": {
        "id": "7jz0nz55Pi1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newmodel = Sequential()\n",
        "#num = 0\n",
        "for i, layer in enumerate(vggmodel.layers):\n",
        "    if i<19:          #Only up to 19th layer to include feature extraction only\n",
        "      newmodel.add(layer)\n",
        "newmodel.summary()"
      ],
      "metadata": {
        "id": "56H06Ss_Pofq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in newmodel.layers:\n",
        "  layer.trainable=False   #We don't want to train these layers again, so False."
      ],
      "metadata": {
        "id": "N4dUaRUFPtCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "VGG16 is expecting an image of 3 dimension with size 224x224 as an input,\n",
        "in preprocessing we have to scale all images to 224 instead of 256\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "UBbfbxg9Px3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'images/colorization/'\n",
        "#Normalize images - divide by 255\n",
        "train_datagen = ImageDataGenerator(rescale=1. / 255)"
      ],
      "metadata": {
        "id": "WUqIjIZUP30r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train_datagen.flow_from_directory(path, target_size=(224, 224), batch_size=32, class_mode=None)"
      ],
      "metadata": {
        "id": "K95m0MOsP7Wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert from RGB to Lab\n",
        "\"\"\"\n",
        "by iterating on each image, we convert the RGB to Lab.\n",
        "Think of LAB image as a grey image in L channel and all color info stored in A and B channels.\n",
        "The input to the network will be the L channel, so we assign L channel to X vector.\n",
        "And assign A and B to Y.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "X =[]\n",
        "Y =[]\n",
        "for img in train[0]:\n",
        "  try:\n",
        "      lab = rgb2lab(img)\n",
        "      X.append(lab[:,:,0])\n",
        "      Y.append(lab[:,:,1:] / 128) #A and B values range from -127 to 128,\n",
        "      #so we divide the values by 128 to restrict values to between -1 and 1.\n",
        "  except:\n",
        "     print('error')\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "X = X.reshape(X.shape+(1,)) #dimensions to be the same for X and Y\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "metadata": {
        "id": "lOz6HXZ4P-8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now we have one channel of L in each layer but, VGG16 is expecting 3 dimension,\n",
        "#so we repeated the L channel two times to get 3 dimensions of the same L channel\n",
        "\n",
        "vggfeatures = []\n",
        "for i, sample in enumerate(X):\n",
        "  sample = gray2rgb(sample)\n",
        "  sample = sample.reshape((1,224,224,3))\n",
        "  prediction = newmodel.predict(sample)\n",
        "  prediction = prediction.reshape((7,7,512))\n",
        "  vggfeatures.append(prediction)\n",
        "vggfeatures = np.array(vggfeatures)\n",
        "print(vggfeatures.shape)"
      ],
      "metadata": {
        "id": "ijeCzeQCQEaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decoder\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(256, (3,3), activation='relu', padding='same', input_shape=(7,7,512)))\n",
        "model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
        "model.add(UpSampling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
        "model.add(UpSampling2D((2, 2)))\n",
        "model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
        "model.add(UpSampling2D((2, 2)))\n",
        "model.add(Conv2D(16, (3,3), activation='relu', padding='same'))\n",
        "model.add(UpSampling2D((2, 2)))\n",
        "model.add(Conv2D(2, (3, 3), activation='tanh', padding='same'))\n",
        "model.add(UpSampling2D((2, 2)))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "s0NQcLWxQJi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='Adam', loss='mse' , metrics=['accuracy'])\n",
        "model.fit(vggfeatures, Y, verbose=1, epochs=10, batch_size=128)\n",
        "\n",
        "model.save('colorize_autoencoder_VGG16.model')"
      ],
      "metadata": {
        "id": "Pbzzro4ZQRr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################################\n",
        "#Predicting using saved model.\n",
        "model = tf.keras.models.load_model('colorize_autoencoder_VGG16_10000.model',\n",
        "                                   custom_objects=None,\n",
        "                                   compile=True)"
      ],
      "metadata": {
        "id": "jstmgcaAQWP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testpath = 'images/colorization2/test_images/'\n",
        "files = os.listdir(testpath)\n",
        "for idx, file in enumerate(files):\n",
        "    test = img_to_array(load_img(testpath+file))\n",
        "    test = resize(test, (224,224), anti_aliasing=True)\n",
        "    test*= 1.0/255\n",
        "    lab = rgb2lab(test)\n",
        "    l = lab[:,:,0]\n",
        "    L = gray2rgb(l)\n",
        "    L = L.reshape((1,224,224,3))\n",
        "    #print(L.shape)\n",
        "    vggpred = newmodel.predict(L)\n",
        "    ab = model.predict(vggpred)\n",
        "    #print(ab.shape)\n",
        "    ab = ab*128\n",
        "    cur = np.zeros((224, 224, 3))\n",
        "    cur[:,:,0] = l\n",
        "    cur[:,:,1:] = ab\n",
        "    imsave('images/colorization2/vgg_result/result'+str(idx)+\".jpg\", lab2rgb(cur))"
      ],
      "metadata": {
        "id": "8IA0fcciQZjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xv813P0iQfm9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}